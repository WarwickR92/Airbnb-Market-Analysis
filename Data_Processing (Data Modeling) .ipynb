{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb - Q2 & Q3 (Boston & Seattle) - Data Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been created to perform the modelling part of the process and to evaluate the model after. As such it will answer the second two bussiness questions as follows:\n",
    "\n",
    "- Can I predict prices based on the market, variables & seasonal trend?\n",
    "- Which AirBnB listing variables have the biggest impact on prices?\n",
    "\n",
    "First I will attempt to create a machine learning model to predict what prices airbnb's should charge when adding a property and then during the evaluation process I will look at feature importance. These two steps should help answer the two business questions above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports basic functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Imports functions for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "import matplotlib.style as style\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import clone \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Imports custom made airbnb package\n",
    "import airbnb_pkg as airbnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data, column, market, ax, binsize=50, y_label='', x_label='', color='blue'):\n",
    "    \"\"\"\n",
    "    Function to plot standard histograms (Can be used for individual or side by side plots)\n",
    "    \"\"\"\n",
    "    # Plots the Seattle Graph with title & axis\n",
    "    sns.distplot(data[column], kde=False, color=color, bins=binsize, ax=ax)\n",
    "    \n",
    "    # Sets up the title based on the plotting column and the market    \n",
    "    ax.set_title(f\"{market} {column} data\", fontsize=25)\n",
    "    \n",
    "    # Sets the y and x labels\n",
    "    ax.set_ylabel(y_label, fontsize=20)\n",
    "    ax.set_xlabel(x_label, fontsize=20)\n",
    "    \n",
    "    # Removes spines and changes layout to tight\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "def plot_line(data, ax, color, alpha=.7, ls='-', lw=5, label=None):\n",
    "    \"\"\"\n",
    "    plots the standard line graph\n",
    "    \"\"\"\n",
    "    ax.plot(data, color=color,\n",
    "            alpha=alpha, ls=ls,\n",
    "            linewidth=lw, label=label)\n",
    "    \n",
    "def outlier_removal(data, column, outlier_pctile=5):\n",
    "    \"\"\"\n",
    "    Removes data from prices outside the 95 percentile\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    upper_theshold = np.percentile(df[column].values, 100-outlier_pctile)\n",
    "    lower_theshold = np.percentile(df[column].values, outlier_pctile)\n",
    "    \n",
    "    df = df[df[column] >= lower_theshold]\n",
    "    df = df[df[column] <= upper_theshold]\n",
    "    \n",
    "    return df      \n",
    "    \n",
    "def validation_metrics(predictions, y_test, baseline_mae='', baseline_mape=''):\n",
    "    # Obtain the absolute erros for each prediction\n",
    "    errors = abs(predictions - test)\n",
    "\n",
    "    print('Mean Absolute error:', round(np.mean(errors), 2))\n",
    "\n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = np.mean(100 * (errors / test))\n",
    "\n",
    "    print('Mean Absolute Percentage error:', round(np.mean(mape), 2))\n",
    "\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - mape\n",
    "    print('Accuracy:', round(accuracy, 2), '%.')\n",
    "    \n",
    "    if baseline_mae:\n",
    "        # Compare to baseline\n",
    "        improvement_baseline = mae - baseline_mae\n",
    "        print('MAPE Improvement:', round(improvement_baseline, 2))    \n",
    "\n",
    "    if baseline_mape:\n",
    "        # Compare to baseline\n",
    "        improvement_baseline = mape - baseline_mape\n",
    "        print('MAPE Improvement:', round(improvement_baseline, 2), '%.')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Setup (Read data, create global variables etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting settings\n",
    "style.use('seaborn-poster') #sets the size of the charts\n",
    "style.use('seaborn-darkgrid')\n",
    "plt.rcParams['font.family'] = \"sans serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the .csv files for Seattle \n",
    "Seattle_Cal = pd.read_csv('Data/Seattle/calendar.csv')\n",
    "Seattle_List = pd.read_csv('Data/Seattle/listings.csv')\n",
    "\n",
    "# Read in the .csv files for Boston \n",
    "Boston_Cal = pd.read_csv('Data/Boston/calendar.csv')\n",
    "Boston_List = pd.read_csv('Data/Boston/listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/warwick.rommelrath/Udacity/Airbnb_Data_Analysis/airbnb_pkg/__init__.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  for x in data['price']]\n",
      "/Users/warwick.rommelrath/Udacity/Airbnb_Data_Analysis/airbnb_pkg/__init__.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['date'] = pd.to_datetime(data['date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns dropped: {'square_feet'}\n",
      "Number of rows dropped: 0\n",
      "Columns dropped: {'square_feet'}\n",
      "Number of rows dropped: 0\n"
     ]
    }
   ],
   "source": [
    "# Create ML input using the function from the previous notebook\n",
    "Seattle_input = airbnb.ML_preprocessing(Seattle_Cal, Seattle_List)\n",
    "Boston_input = airbnb.ML_preprocessing(Boston_Cal, Boston_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with I will run the modelling on the seattle data alone and then the Boston data will be ran to confirm the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could apply PCA at this point but for the sake of looking into which variables are the most important I'm going to keep my data how it is (I will try PCA later on in this notebook after my initial analysis. \n",
    "\n",
    "For the ML I've decided to test the random forests algorthim for a few reasons:\n",
    "- It's suited towards creating regression models with many features.\n",
    "- It will be quick to train and get results which will be useful for a few reasons e.g. re-training on new markets, easily deployed in the workplace, keeps computational costs down.\n",
    "- It's useful to look at feature importance.\n",
    "\n",
    "The downsides potentially are:\n",
    "- Although its much less prone than decision trees it can overfit occationally\n",
    "- Some of the feature importance outputs may be skewed towards the catagorical variables in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the X & Y variables\n",
    "X = Seattle_input.iloc[:,1:]\n",
    "Y = Seattle_input.iloc[:,0].values\n",
    "\n",
    "# split into training & testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimised Model\n",
      "------\n",
      "RMSE Train Score: 0.9110337329607967\n",
      "RMSE Test Score: 0.6097888546654754\n"
     ]
    }
   ],
   "source": [
    "# setup the random forests classifier\n",
    "rf = RandomForestRegressor(oob_score=True, bootstrap=True, random_state=14)\n",
    "\n",
    "# specify parameters for grid search\n",
    "parameters = {\n",
    "    'n_estimators': [1,2,10,50,100,200],\n",
    "    'min_samples_leaf':[1,2,3],\n",
    "    'min_samples_split': [2,3,4],\n",
    "}\n",
    "\n",
    "# create grid search object\n",
    "reg = GridSearchCV(rf, param_grid=parameters, n_jobs=-1, verbose=2, cv=5)\n",
    "\n",
    "# train classifier\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = reg.best_estimator_.predict(X_test)\n",
    "\n",
    "print(\"\\nOptimised Model\\n------\")\n",
    "print(f'RMSE Train Score: {reg.best_estimator_.score(X_train, y_train)}')\n",
    "print(f'RMSE Test Score: {reg.best_estimator_.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy score above is around RMSE  error is 91% accurate which is pretty good but the testing score of 69% is pretty bad showing that the model is acctually overfitting. One possible cause of this could be due to the amount of features we are trying to base the modelling on as we have 100+ all trying to influence it.\n",
    "\n",
    "It's also worth pointing out that calculating the RMSE has the benefit of penalizing large errors but in this case it's possible that using MAE is a better function as it might be fine that some of the data points (the outliers) are way off from our prediction. Given this is might be worth removing these and re-testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average absolute error: 27.6\n",
      "Mean Absolute error: 27.6\n",
      "Mean Absolute Percentage error: 24.93\n",
      "Accuracy: 75.07 %.\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy based on MAE instead of RMSE\n",
    "\n",
    "# Obtain the absolute erros for each prediction\n",
    "errors = abs(y_pred - y_test)\n",
    "\n",
    "print('Average absolute error:', round(np.mean(errors), 2))\n",
    "\n",
    "# Calculate mean absolute percentage error (MAE)\n",
    "mae = np.mean(errors)\n",
    "\n",
    "print('Mean Absolute error:', round(np.mean(mae), 2))\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = np.mean(100 * (errors / y_test))\n",
    "\n",
    "print('Mean Absolute Percentage error:', round(np.mean(mape), 2))\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - mape\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "\n",
    "# Save the model above as a baseline to check improvement\n",
    "mae_baseline = mae\n",
    "mape_baseline = mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from calculating the MAE the absolute error is around 27.6 with a percentage error of 24.9% which again isn't perfect but isn't as skewed by outliers as by the RMSE. I will now remove the extreme outliers and re-create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done 166 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimised Model\n",
      "------\n",
      "RMSE Train Score: 0.8701781253399095\n",
      "RMSE Test Score: 0.6227280432472169\n",
      "Average absolute error: 26.62\n",
      "Mean Absolute error: 26.62\n",
      "Mean Absolute Percentage error: 24.76\n",
      "Accuracy: 75.24 %.\n",
      "MAPE Improvement: -0.98\n",
      "MAPE Improvement: -0.17 %.\n"
     ]
    }
   ],
   "source": [
    "# Remove only the most extreme outliers\n",
    "Seattle_ro = outlier_removal(Seattle_input, 'Cal_price', outlier_pctile=5)\n",
    "\n",
    "# Set the X & Y variables\n",
    "X = Seattle_ro.iloc[:,1:]\n",
    "Y = Seattle_ro.iloc[:,0].values\n",
    "\n",
    "# split into training & testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# setup the random forests classifier\n",
    "rf = RandomForestRegressor(oob_score=True, bootstrap=True, random_state=14)\n",
    "\n",
    "# specify parameters for grid search\n",
    "parameters = {\n",
    "    'n_estimators': [1,2,10,50,100,200],\n",
    "    'min_samples_leaf':[1,2,3],\n",
    "    'min_samples_split': [2,3,4],\n",
    "}\n",
    "\n",
    "# create grid search object\n",
    "reg = GridSearchCV(rf, param_grid=parameters, n_jobs=-1, verbose=2, cv=5)\n",
    "\n",
    "# train classifier\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = reg.best_estimator_.predict(X_test)\n",
    "\n",
    "print(\"\\nOptimised Model\\n------\")\n",
    "print(f'RMSE Train Score: {reg.best_estimator_.score(X_train, y_train)}')\n",
    "print(f'RMSE Test Score: {reg.best_estimator_.score(X_test, y_test)}')\n",
    "\n",
    "validation_metrics(y_pred, y_test, baseline_mae=mae_baseline, baseline_mape=mape_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the outliers increased the accuracy slightly but the model is now probably not been as skewed by the outlier datapoints. The next step will be to perform fetaure scaling, this will ensure that variables with a bigger varience don't have a bigger influence on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   28.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimised Model\n",
      "------\n",
      "RMSE Train Score: 0.9112129381772082\n",
      "RMSE Test Score: 0.6041080352465447\n",
      "Average absolute error: 27.28\n",
      "Mean Absolute error: 27.28\n",
      "Mean Absolute Percentage error: 23.3\n",
      "Accuracy: 76.7 %.\n",
      "MAPE Improvement: -0.32\n",
      "MAPE Improvement: -1.63 %.\n"
     ]
    }
   ],
   "source": [
    "# Perform feature scaling on the input data\n",
    "StdSclr = StandardScaler()\n",
    "Seattle_sc = pd.DataFrame(StdSclr.fit_transform(Seattle_ro))\n",
    "\n",
    "# Set the X & Y variables\n",
    "X = Seattle_sc.iloc[:,1:]\n",
    "Y = Seattle_ro.iloc[:,0].values\n",
    "\n",
    "# split into training & testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# setup the random forests classifier\n",
    "rf = RandomForestRegressor(oob_score=True, bootstrap=True, random_state=14)\n",
    "\n",
    "# specify parameters for grid search\n",
    "parameters = {\n",
    "    'n_estimators': [1,2,10,50,100,200],\n",
    "    'min_samples_leaf':[1,2,3],\n",
    "    'min_samples_split': [2,3,4],\n",
    "}\n",
    "\n",
    "# create grid search object\n",
    "reg = GridSearchCV(rf, param_grid=parameters, n_jobs=-1, verbose=2, cv=5)\n",
    "\n",
    "# train classifier\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = reg.best_estimator_.predict(X_test)\n",
    "\n",
    "print(\"\\nOptimised Model\\n------\")\n",
    "print(f'RMSE Train Score: {reg.best_estimator_.score(X_train, y_train)}')\n",
    "print(f'RMSE Test Score: {reg.best_estimator_.score(X_test, y_test)}')\n",
    "\n",
    "validation_metrics(y_pred, y_test, baseline_mae=mae_baseline, baseline_mape=mape_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetaure scaling improved the RMSE error but acctually slightly made the MAE error slightly worse. This shows that the modelling improved on some extremes but not on the data as a whole and might suggest using a bigger outlier removal.\n",
    "\n",
    "\n",
    "In order to improve the model further we should look to how each feature is contributing to the model. The best way to find which features are contributing most to the model would be to re-train the model while dropping one feature each at a time and then recomputing the model scores. This may be compuationally intestive compaired to some other methods but it will clearly show what features are driving the model and which ones are making it worse. First I will re-run the original model with the best parameters as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=2, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                      n_jobs=None, oob_score=True, random_state=14, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimised Model\n",
      "------\n",
      "RMSE Train Score: 0.9126322214631966\n",
      "RMSE Test Score: 0.5645713622896062\n",
      "Average absolute error: 0.46\n",
      "Mean Absolute error: 0.46\n",
      "Mean Absolute Percentage error: -78.06\n",
      "Accuracy: 178.06 %.\n",
      "MAPE Improvement: -27.15\n",
      "MAPE Improvement: -102.99 %.\n"
     ]
    }
   ],
   "source": [
    "def RF_price_model(X_model_input, Y_model_input):\n",
    "    \"\"\"\n",
    "    Creates the price regression Random Forests based on the desired input df\n",
    "    (Expects dependent variable to be the first column)\n",
    "    \"\"\"\n",
    "    # Set the X & Y variables\n",
    "    X = X_model_input.iloc[:,1:]\n",
    "    Y = Y_model_input.iloc[:,0].values\n",
    "\n",
    "    # split into training & testing data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    \n",
    "    # setup the random forests classifier\n",
    "    rf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=2, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                      n_jobs=None, oob_score=True, random_state=14, verbose=0,\n",
    "                      warm_start=False)\n",
    "\n",
    "    # train classifier\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # predict on test data\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    print(\"\\nOptimised Model\\n------\")\n",
    "    print(f'RMSE Train Score: {rf.score(X_train, y_train)}')\n",
    "    print(f'RMSE Test Score: {rf.score(X_test,y_test)}')\n",
    "    \n",
    "    validation_metrics(y_pred, y_test, baseline_mae=mae_baseline, baseline_mape=mape_baseline)\n",
    "    \n",
    "\n",
    "best_model = RF_price_model(Seattle_sc, Seattle_ro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our best model I can run it through the feature importance function I've created that loops through each column and removes it from the input prior to creating the model. If we compaire this to the accuracy of the core model we will have an idea of which columns are having a positive or negative influence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model, X_train, y_train, input_data, rand_state):\n",
    "    \"\"\"\n",
    "    This module calculates the feature importance of all columns\n",
    "    \"\"\"\n",
    "    # Creates a list for storing the importance info\n",
    "    feat_imp = []\n",
    "    \n",
    "    # clone the model\n",
    "    mod_clone = clone(model)\n",
    "    \n",
    "    # set random_state, train and score the benchmark model\n",
    "    mod_clone.random_state = rand_state\n",
    "    mod_clone.fit(X_train, y_train)\n",
    "    mod_score = mod_clone.score(X_train, y_train)\n",
    "    \n",
    "    # Loop through columns creating feature importance score    \n",
    "    for col in X_train.columns:\n",
    "        # Drop the column we are calulating the influence of         \n",
    "        X_input = X_train.drop(columns=[col])\n",
    "        \n",
    "        # Repeat the process of cloning, random state, train and score         \n",
    "        new_mod = clone(model)\n",
    "        new_mod.random_state = rand_state\n",
    "        new_mod.fit(X_input, y_train)\n",
    "        update_score = new_mod.score(X_input, y_train)\n",
    "        \n",
    "        # Calulate the feature importance and append to list\n",
    "        feat_imp.append(mod_score - update_score)\n",
    "    \n",
    "    # Create the output that shows each columns feature importance     \n",
    "    importances_df = pd.DataFrame(feat_imp, index=input_data.columns)\n",
    "    \n",
    "    return importances_df\n",
    "\n",
    "importance = feature_importance(best_model, X_train, y_train, Seattle_ro.iloc[:,1:], 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(feat_importance):\n",
    "    \"\"\"\n",
    "    This plot is designed to plot the feature performance in the model\n",
    "    \"\"\"\n",
    "    # It's probably easier to represent this data graphically\n",
    "    fig, ax = plt.subplots(figsize=(18,9))\n",
    "\n",
    "    # Sorts the data\n",
    "    sorted_imp = feat_importance.sort_values(0, ascending=False)\n",
    "    \n",
    "    # Select the X & Y values\n",
    "    x = range(1,len(sorted_imp)+1)\n",
    "    y = sorted_imp.values\n",
    "\n",
    "    # Creates a vertical line graph\n",
    "    import seaborn as sns\n",
    "    ax.vlines(x=x, ymin=0, ymax=y, color='blue', alpha=0.4)\n",
    "    ax.scatter(x, y, color='blue', s=6, alpha=1)\n",
    "\n",
    "    # Format the X-axis\n",
    "    plt.xticks(range(1,len(sorted_imp)+1), list(sorted_imp.index), rotation=89, fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plot_feature_importance(importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its odd that the highest performing variable is 'bathrooms' some of the other high values do make sense like 'accomodates' & 'Entire home/apt'. The results show that alot of the variables that I'm putting into the model are acctually having a negative effect on the outcome and might as well be ignored.\n",
    "\n",
    "At this point it might be worth dropping some of the values that are having a negative influence on the model or at least the worst perfoming ones and running the analysis & potentially re-running the model to see if they both improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the colums that are below zero\n",
    "col_drop = list(importance[importance[0] < 0].index)\n",
    "\n",
    "# Drop the unimportant columns\n",
    "Seattle_drop = Seattle_ro.drop(columns=col_drop)\n",
    "\n",
    "# Perform feature scaling on the input data\n",
    "StdSclr = StandardScaler()\n",
    "Seattle_sc_update = pd.DataFrame(StdSclr.fit_transform(Seattle_drop))\n",
    "\n",
    "# Re-crea\n",
    "new_rf = RF_price_model(Seattle_sc_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the feature importance\n",
    "importance_updated = feature_importance(new_rf, X_train, y_train, Seattle_ro.iloc[:,1:], 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-do the graph\n",
    "plot_feature_importance(importance_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the accuracy of the model hasn't impoved it's starting to make more sense as accommodates is now top which seems more likely than bathrooms. It might be worth removing the new features that are below zero as they may help the model even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the colums that are below zero\n",
    "col_drop = list(importance[importance[0] < 0].index) + list(importance_updated[importance_updated[0] < 0].index)\n",
    "\n",
    "# Create the new inputs without these columns\n",
    "Update_input = Seattle_input.drop(columns=col_drop)\n",
    "\n",
    "# Re-crea\n",
    "update_rf = RF_price_model(Seattle_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the feature importance\n",
    "importance_updated_2 = feature_importance(rf, X_train, y_train, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-do the graph\n",
    "plot_feature_importance(importance_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "errors = abs(predictions - y_test)\n",
    "\n",
    "print('Metrics for Random Forest Trained on Expanded Data')\n",
    "print('Average absolute error:', round(np.mean(errors), 2), 'degrees.')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = np.mean(100 * (errors / y_test))\n",
    "\n",
    "# Compare to baseline\n",
    "improvement_baseline = 100 * abs(mape - baseline_mape) / baseline_mape\n",
    "print('Improvement over baseline:', round(improvement_baseline, 2), '%.')\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - mape\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
